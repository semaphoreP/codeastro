{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Profiling : Analyze the cProfile output\n",
    "\n",
    "After you have run the `profile_orbitize.py` function with the cProfile package, we will use the `pstats` module to read and analyze the output of the profiler and identify bottlenecks in the code. If you are in Colaboratory, you will need to upload the output of the cProfile to Colaboratory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats\n",
    "mystats = pstats.Stats(\"profiler_output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the top 15 functions that took the longest time to run\n",
    "\n",
    "Even in such a short script, python called millions of function! A lot of them are super quick and really not what we are after. Typically, we want to find the few functions that are bottlenecks in our code.\n",
    "\n",
    "The function below sorts the functions by which took the longest to run, and prints out only the stats of the top 15 functions that took longest to run. The columns below mean:\n",
    "  * ncalls: number of times this function was called\n",
    "  * tottime: the total execution time spent in this code NOT including calls to other functions\n",
    "  * percall: this first percall divides tottime by ncalls. The amount of time per call spent solely in this function.\n",
    "  * cumtime: the total execution time spent in this code INCLUDING calls to sub functions. cumtime > tottime always.\n",
    "  * percall: second percall divides cumtime by ncalls\n",
    "  * filename: the name of the function being considered\n",
    "\n",
    "Both `tottime` and `cumtime` are useful. A function with a high `tottime` means we should focus on speeding up this function. A function with only a high `cumtime` means we should see what this function is calling to improve runtime.\n",
    "  * `_newton_solver()` is a function with a high tottime. We should look at the lines in that function to check out how to speed it up.\n",
    "  * `compute_model()` is a function with a low tottime but a very high `cumtime`. We should look at the functions it calls to figure out what takes up so much time.\n",
    "\n",
    "Generally, we want to put some filters on `print_stats`, becuase otherwise there will be so much printed out it is unmanageable.\n",
    "\n",
    "For more features: [pstats documentaiton](https://docs.python.org/3/library/profile.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystats.sort_stats(\"time\").print_stats(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply multiple filters. For example let's look at the top 15 numpy functions that took the most time to run. Note that the order of the filtering matters. This command first selects every function with numpy in the name, followed by taking the top 15. Calling `print_stats(15, 'numpy')` would pick the top 15 longest runtime functions, and downselecting which has numpy in the name from those. That would give us less than 15 numpy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystats.sort_stats(\"time\").print_stats('numpy', 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity\n",
    "\n",
    "## Roles\n",
    "  * Driver: in charge of sharing their screen and typing the code for this activity\n",
    "  * Recorder: in charge of writing down all the results and posting them on Piazza\n",
    "  * Navigator: in charge of directing the driver what to code (can be combind with recorder)\n",
    "\n",
    "## Instructions:\n",
    "  * Go through each step and post your answers on Piazza.\n",
    "\n",
    "## Activity Part 1: Analyze the output\n",
    "\n",
    "Generate the output above on your own by running `python -m cProfile -o profiler_output.txt profile_orbitize.py` and answer the following questions by analyzing the output.\n",
    "\n",
    "1. Which function takes up the most runtime (not including calls to sub-functions)?\n",
    "\n",
    "2. Which function takes up the most runtime (including calls to sub-functions)?\n",
    "\n",
    "3. Which function is called the most? Which `orbitize` function is called the most?\n",
    "\n",
    "4. If we had the magical ability to speed up one function by a factor of 2, which function should we speed up? What is the improvement in end-to-end runtime of the script?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Part 2: Investigate why `_logl` takes so long\n",
    "\n",
    "`_logl()` is a helper function in `orbitize!` to compute the log likelihood of the data given the model. The `_logl()` function itself has a short runtime but it is calling something that takes a long time that makes it have a long `cumtime`. We can use the `print_callees()` function to look at the stats of all the functions it calls.\n",
    "\n",
    "We can see that `compute_model` is the function with the highest cumtime, but its tottime is low, so we something it calls takes all the time. We must dig deeper! Keep digging down recursively to find what function called within `_logl()` that takes the longest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystats.print_callees('_logl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Part 3: Which function calls `numpy.array` the most?\n",
    "\n",
    "`numpy.array` is a popular function because it gets called anytime a new numpy array gets created. We can use `print_callers()` to see which functions call it to look into potentially reducing the number of array creations to speed up the code. Which function in `orbitize` calls `numpy.array` the most times per function call? (it's harder than it looks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystats.print_callers('numpy.array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Profiling Strategy\n",
    "\n",
    "![](imgs/code_speedup.png)\n",
    "\n",
    "Generally, it is important to focus your effort on speeding up the 1 or 2 slowest functions. If you speed up something that takes 50% of the runtime by a factor of 2, you speed up the code by a factor of 25%. If a piece of code takes 10% of the runtime, a factor of 2 improvement (which by itself is often hard to get) only gives you a 5% improvement in the end. **Focus your effort on optimizing the runtime of the one or two functions that take the most amount of time.**\n",
    "\n",
    "For all else, it is generally more important to optimize code readibility over runtime. Code that is more readible is less prone to bugs and eaiser to maintain. If a piece of code takes only 0.0001% of the runtime, any amount of speed up is not worth making the code hard to read (from a person-hour perspective). Many times, it is difficult to gain more than a factor of 2 speed up in runtime. Definitely think about the potential gain in speed-up versus how long it takes to write and maintian the code.\n",
    "\n",
    "## Some Ways to Speed up Your Code\n",
    "\n",
    "If you identified the critical chunk of code to speed up, the general strategy is to remove unncessary computations as much as possible. There's unfortunately not one single method to speed up your code. But here are a few ideas:\n",
    "\n",
    "1. Eliminate computations that are not being used (e.g., computing variables you do not use; processing parts of images that will be discarded). \n",
    "2. Use `numpy` functions whenever possible for matrix operations\n",
    "3. Avoid using python `for` and `while` loops as they are slow\n",
    "4. If MKL/BLAS is being run inside of already-parallelized code, disable MKL/BLAS (see Day4 parallelization materials for more details on MKL/BLAS).\n",
    "5. Reduce the number of iterations or increase the tolerance of routines that are unncessarily precise (e.g., optimizers can run for less iterations; sin and cos can be approximated by taylor expansions). \n",
    "6. Avoid copying variables when they do not need to be copied (e.g., if the input is already a numpy.array, you don't need to wrap it with `np.array()` in ensure it's a numpy array). \n",
    "7. Turn some of your python code into C-code and call it with Python\n",
    "8. Other ideas?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Performance Extrapolation\n",
    "\n",
    "If your code takes 24 hours to run to completion, or needs to be run on a cluster, it is likely still easier to develop and benchmark it on your laptop. We can use extrapolation to estimate performance. \n",
    "\n",
    "### Runtime\n",
    "\n",
    "Generally algorithm runtimes scale as N^a where a is some positive number greater than 1, and N is the size of your data. (In CS terms, this is often disccused in \"Big O\" notation as  $O(N^a)$ runtime). If your data is too big or takes too long to benchmark on a single machine, try running your code on much smaller data. If you do this for ~3-5 different test data with different N, you can estimate what is the scaling of your code (i.e., what is \"a\"). Once you know the scaling you can use that scaling relation to extrapolate how long it will take on larger datasets. If you can also use more cores, you can divide the final runtime by the number of cores you can use. \n",
    "\n",
    "For example (very idealized), my code takes 1 second on 10x10 data, 100 seconds on 100x100 data, and 400 seconds on 200x200 data (all on a single core). My code scales as N^2 where N is the size of one dimension of the data. If I want to run it on 10000x10000 data with 100 cores, my code should take (10000^2)/100 = 1,000,000 seconds to run. In practice there will probably be some deviations to these estimates (due to overheads, CPU usage scheudling, other factors), but this scaling should give you a general sense of how long something will take. \n",
    "\n",
    "### Memory\n",
    "\n",
    "If you are worried that your program will run out of memory, try calculating how much memory your program uses. On modern 64-bit machines, a single number (an integer, float) takes up 8 bytes. If you have an array of 1000x1000 floats, then it will take up 8e6 bytes or 8 MBs. If you have 3-D numerical data with dimensions 1000x1000x1000, then it will take up 8e9 bytes or 8 GBs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70da8eaca2f829bebd9ae4bfee73e2015b5a57abadfaaa95753587bc60143f91"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('codeastro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
