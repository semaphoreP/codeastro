{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism\n",
    "\n",
    "Computers nowadays all have multiple cores that can run code in parallel. Many codes and algorithms in astronomy can be categorized as \"embarrassingly parallel\", meaning that they can easily be split up into multiple tasks that each have little or no dependency on each other. An example of this is running the same image processing steps on multiple files, or computing the likelihood of a bunch of different sets of models. \n",
    "\n",
    "Parallelism adds extra complexity to the code, making it harder to debug and maintain. Thus, it is important to consider the relative gain of putting in parallelism versus the extra effort in developing and maintaining that code. For this reason, we also recommend that you keep parallelization code as simple as possible. For many tasks, even the most simple parallelism is sufficient. \n",
    "\n",
    "We will discuss a couple of options to do parallelism in Python and mention a few other tips:\n",
    "\n",
    "  1. BLAS/LAPACK\n",
    "  2. Thread/process pools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.ndimage as ndimage\n",
    "import timeit\n",
    "import multiprocessing\n",
    "import astropy.io.fits as fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLAS/LAPACK/ATLAS/MKL\n",
    "\n",
    "There are packages that automatically parallelize linear algebra routines like matrix dot product or singular value decomposition. BLAS/LAPACK are the algorithms, and ATLAS, OpenBLAS, or MKL are implementations of them. If you have an Intel processor, you will be using MKL. They are quite useful for when you don't want to write your own parallelization code but want to speed up matrix routines. It requires numpy to be configured for BLAS/MKL properly. Generally if you install numpy with Anaconda, this happens automatically, so we recommend simply doing it this way. These speedups are turned on by default when configured.\n",
    "\n",
    "However, if you wish to do your own parallelism, make sure to turn this off. Having multiple layers of parallelism will often slow down your code! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check we have BLAS/LAPACK. Notice they are from the MKL library\n",
    "np.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Resource Usage\n",
    "\n",
    "Anytime you are testing parallelized code, it is code to monitor your CPU/RAM usage. Monitoring your CPU usage can help you assess if the number of processes being run in parallel is consistent with what you are looking for. Many times, running parallelized code already involves big data sets, and parallelization will use even more memory (you can think of it as trading runtime for memory usage). So have your resource monitor up occasionally when you are developing this code. It is also a great way to debug the code (e.g., identify hanging parallelization that is not finishing). \n",
    "\n",
    "In the code block below, we are using the MKL package which works with Intel CPUs. If you do not have an Intel CPU, this command will not do anything. You would need to configure BLAS with environment variables (MKL can be done this way too, but it affects your entire user, rather than just this python script locally). Here's an example to modify it in bash to turn off BLAS parallelization:\n",
    "\n",
    "    export OPENBLAS_NUM_THREADS=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mkl\n",
    "\n",
    "mat = np.random.random((1000, 1500)) # 1000x1500 matrix\n",
    "\n",
    "# Use 4 cores to parallelize matrix operations\n",
    "mkl.set_num_threads(4)\n",
    "print(\"4 cores\", timeit.timeit(\"mat.dot(mat.T)\", setup=\"from __main__ import mat\", number=100))\n",
    "\n",
    "# Disable MKL\n",
    "mkl.set_num_threads(1)\n",
    "print(\"1 core\", timeit.timeit(\"mat.dot(mat.T)\", setup=\"from __main__ import mat\", number=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Multiple Python Instances\n",
    "\n",
    "This might sound very unsophisticated, but it is actually a good choice for many instances. As you will learn before, Python parallelism is not the best, and if tasks are completely independent of each other, running each task as a separate Python process and saving the result as a file is a perfectly reasonable (and simple) option. This is great for batch processes such as bulk processing a bunch of files with some data reduction code.\n",
    "\n",
    "There are many options to do this: bash script, GNU Parallel, or, as we will focus on today, a master python script. GNU Parallel is also quite useful and is discussed at the end of this notebook. We will focus on using a python script to launch a bunch of python processes because all capabilities of shell scripting (e.g., calling bash commands with the sys module) can be done in Python, and often with much better readability. We will use python to launch a bunch of python processes.\n",
    "\n",
    "We create a function with an argument `index` that tells each proces what chunk of the task to run. We then create a bunch of processes, give them their chunks, and call `start()` to run them. Afterwards, our master process uses `join()` to wait for each process to finish. It is important to always call `join()` at the end to ensure all processes have finished running! If a process has finished immediately, `join()` will immediately return; if a process has not finished, our master process will sleep until the process it is waiting on has finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def matrix_loop(mat, index):\n",
    "    # divide up so that we only compute one chunk of the mat.dot(mat.T) matrix\n",
    "    index_start = mat.shape[0] // 10 * index\n",
    "    index_end = mat.shape[0] // 10 * (index + 1)\n",
    "    val  = mat[index_start:index_end].dot(mat.T)\n",
    "    print(\"Process {0} complete\".format(index))\n",
    "    # could save value to a shared variable or save to a file to be used later\n",
    "\n",
    "process_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    p = multiprocessing.Process(target=matrix_loop, args=(mat, i))\n",
    "    process_list.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in process_list:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pools\n",
    "\n",
    "Manually creating threads requires a bunch of upkeep code, which is unnecessary if you are just running over a giant loop. In the spirit of keeping parallelism simple, use a high-level parallelization API provided by your programming language whenever possible! It will save you time and effort (Trust me). For dividing up tasks with a for loop, use Python process `Pools`. Essentially, you can give\n",
    "any number of tasks to a process `Pool` and the processes in the pool will loop through and do each one per your instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(processes=4) # creae a pool with 4 worker processes\n",
    "\n",
    "def matrix_loop(mat, index):\n",
    "    # divide up so that we only compute one chunk of the mat.dot(mat.T) matrix\n",
    "    index_start = mat.shape[0] // 10 * index\n",
    "    index_end = mat.shape[0] // 10 * (index + 1)\n",
    "    val  = mat[index_start:index_end].dot(mat.T)\n",
    "    print(\"Process {0} complete\".format(index))\n",
    "    \n",
    "    return val # let's return the data this time\n",
    "\n",
    "pool_jobs = []\n",
    "for i in range(10):\n",
    "    job = pool.apply_async(matrix_loop, (mat, i))\n",
    "    pool_jobs.append(job)\n",
    "    print(\"Created job {0}\".format(i))\n",
    "\n",
    "for i, job in enumerate(pool_jobs):\n",
    "    result = job.get() \n",
    "    print(\"Result {0}\".format(i), result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Image Processing\n",
    "\n",
    "Run the following code to generate some fake data and then run the following image processing function on each fake image and save the resulting data. How fast can you process the data when the images are 1000x1000? What is the speedup from 1 core to multiple cores (is it linear?)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, this cell might take a minute to run.\n",
    "\n",
    "def generate_fake_data(filename, dims):\n",
    "    \"\"\"\n",
    "    Generates a fake dataframe with random numbers\n",
    "    \n",
    "    Args:\n",
    "        filename (str): file to save the data to\n",
    "        dims (tuple): (Ny, Nx) pair that species the size of the y and x dimensions\n",
    "    \"\"\"\n",
    "    # some complicated random image generation. Feel free to ignore.\n",
    "    # coordinate system in fourier spae\n",
    "    u,v = np.meshgrid(np.fft.fftfreq(dims[1]), np.fft.fftfreq(dims[0]))\n",
    "    phases = np.random.uniform(0, 2*np.pi, u.shape)\n",
    "    rho = np.sqrt((u*dims[1])**2 + (v*dims[0])**2)\n",
    "    # suppress high frequency by a squared exponential\n",
    "    spectrum = np.exp(-rho**2/(np.max(rho)/50)**2)  * np.exp(1j * phases)\n",
    "    filtered = np.real(np.fft.ifft2(spectrum))\n",
    "\n",
    "    fits.writeto(filename, filtered, overwrite=True)\n",
    "\n",
    "\n",
    "# generate fake data\n",
    "fileformat = os.path.join(\"../example_data\", \"fake_{0}x{1}_{2}.fits\")\n",
    "\n",
    "ny = 1000\n",
    "nx = 1000\n",
    "for i in range(25):\n",
    "    filename = fileformat.format(ny, nx, i)\n",
    "    generate_fake_data(filename, (ny, nx) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(frame, filtersize=50):\n",
    "    \"\"\"\n",
    "    Run a high-pass filter on the data. \n",
    "    Remove the low spatial frequency (i.e., smooth features) in the image\n",
    "\n",
    "    Args:\n",
    "        frame (np.array): a 2-D image to be processed\n",
    "        fitersize (int): the size of the filter. Features smaller than the filtersize will be preserved\n",
    "\n",
    "    Returns:\n",
    "        processed_frame (np.array): a 2-D image after processing\n",
    "    \"\"\"\n",
    "    # run a median filter to smooth the image\n",
    "    frame_smooth = ndimage.median_filter(frame, filtersize)\n",
    "\n",
    "    processed_frame = frame - frame_smooth\n",
    "\n",
    "    return processed_frame\n",
    "\n",
    "# an example of running this on one image\n",
    "with fits.open(fileformat.format(ny, nx, 0)) as hdulist:\n",
    "    data = np.copy(hdulist[0].data)\n",
    "\n",
    "\n",
    "    filt_data = process_image(data)\n",
    "\n",
    "    fig = plt.figure(figsize=(6,3))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.imshow(data, cmap=\"inferno\")\n",
    "    ax1.set_title(\"Original\")\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.imshow(filt_data, cmap=\"inferno\")\n",
    "    ax2.set_title(\"Filtered\")\n",
    "\n",
    "#### Activity:\n",
    "# write and time some code that does this in parallel. How does the performance increase as you increase the number of processes you use?\n",
    "# we recommend you use multiprocessing pool for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism Extended Cut\n",
    "\n",
    "There is a lot of talk about in parallelism - we are just scratching the surface. The above concepts _should_ cover the majority of parallelism use cases. However, here are some good things to know.\n",
    "\n",
    "\n",
    "## Do you really need parallelism\n",
    "\n",
    "We already emphasized this above, but parallelism adds complexity. Try avoiding parallelization until it is necessary. If some code takes 10 minutes to run, but you only need to run it once per week, is it worth parallelizing? The programming and upkeep costs may not be worth it. Parallelism also makes it hard to debug: we cannot attach the python debugger on other Python processes meaning they often crash with no error message as to why. GNU Parallel (below) is an alternative to writing more complicated code.\n",
    "\n",
    "## GNU Parallel\n",
    "\n",
    "If you have a script that you want to run multiple times (e.g., on multiple files), you don't need to write Python parallelization. We can use the `parallel` package that can be installed on UNIX systems to handle the parallelization. For example, if we can process a single file by running on the command line:\n",
    "\n",
    "    python process.py file_01.fits\n",
    "\n",
    "Then we can run it on `file_01.fits` to `file_99.fits` by running on the command line:\n",
    "\n",
    "    parallel python process.py ::: file_{00..99}.fits\n",
    "\n",
    "\n",
    "## Threads vs Processes\n",
    "Python has two parallelization modules: `multiprocessing` and `multithreading`. What's the difference? Why do we only use `multiprocessing`?\n",
    "\n",
    "Threads and processes are both tasks that your computer CPUs run in parallel. Threads share the same memory, whereas processes do not. Processes have to communicate themselves with shared variables (see below) or via a slower I/O method (writing to files, communicating over websockets). Most languages use threads to parallelize computations because sharing the same memory is very convenient and saves on resources. However, Python has the \"Global Intepreter Lock\" (GIL) that allows only one thread to run at a time (for complciated consistency reasons). Generally, parallelism in astronomy involves computationally intensive tasks so only one of them being able to run at a time would defeat the purpose. That's why you will generally only see multiprocessing in astronomy-related software development.\n",
    "\n",
    "Threads are more frequently seen outside of astronomy, but it is generally useful for \"I/O bound tasks\" as opposed to the \"CPU bound tasks\" that we usually encounter in astronomy. Anytime you have tasks with a lot of sleeping/waiting time such as if you have multiple web API queries (e.g., multiple database queries) or waiting for files to be created, threads are a better choice because they use less memory and they can access all of your variables instead of having to define shared variables.\n",
    "\n",
    "The one notable exception is that `numpy` functions that call C code releases the GIL. This means if your code is dominated by `numpy` matrix operations such as dot product, then you can actually use threads with minimal increase in runtime.\n",
    "\n",
    "## Shared Variables\n",
    "\n",
    "Processes do not by default share memory. For processes to read/write/access the same variables, we have to declare shared memory. Python `multiprocessing` provides nearly all shared memory structure that you should be using: queues and arrays. Queues allow for interprocess communication. Arrays allow for large datasets to be accessed by multiple processes without needing to duplicate them (possibly saving a lot of memory usage). Use these special arrays and queues becasue they are automatically synchronized between processes and do not require synchronization code, which is generally low-level synchronization that we should avoid programming unless we absolutely need it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}