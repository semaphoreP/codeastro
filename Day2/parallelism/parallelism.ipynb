{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism\n",
    "\n",
    "<img src=\"opt_code.jpg\"/>\n",
    "\n",
    "Your CPU has multiple cores, but that still wouldn't mean that it would run Hogwarts Legacy at max settings, but eh, it can atleaast run code in parallel. Many codes and algorithms in astronomy can be categorized as \"embarrassingly parallel\", meaning that they can easily be split up into multiple tasks that each have little or no dependency on each other. An example of this is running the same image processing steps on multiple files, or computing the likelihood of a bunch of different sets of models. \n",
    "\n",
    "Parallelilization adds extra complexity to the code, making it harder to debug and maintain. And you don't want that when you're already stressed about thinking what to come up to tell your advisor. Thus, it is important to consider the relative gain of implementing parallelization versus the extra effort in developing and maintaining that code. For this reason, we also recommend that you keep parallelization code as simple as possible. For many tasks, even the most simple parallelism is sufficient. \n",
    "\n",
    "We will discuss the 2 main parallelism protocols, how to implement parallelism in Python mainly with thread/process pools and a bit about GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.ndimage as ndimage\n",
    "import timeit\n",
    "import astropy.io.fits as fits\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "def find_brightest_point(image):\n",
    "    \"\"\"Find the brightest point in a 2D numpy array representing an image.\"\"\"\n",
    "    y, x = np.unravel_index(np.argmax(image), image.shape)\n",
    "    brightness = image[y, x]\n",
    "    return x, y, brightness\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data: a list of 10 random images\n",
    "    num_images = 10\n",
    "    image_size = (100, 100)\n",
    "    images = [np.random.random(image_size) for _ in range(num_images)]\n",
    "    \n",
    "    # Create a pool of worker processes\n",
    "    pool = multiprocessing.Pool(processes=4)  # Adjust the number of processes as needed\n",
    "    \n",
    "    # Map the find_brightest_point function to the list of images\n",
    "    results = pool.map(find_brightest_point, images)\n",
    "    \n",
    "    # Close the pool and wait for the work to finish\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Print the results\n",
    "    for i, (x, y, brightness) in enumerate(results):\n",
    "        print(f\"Image {i}: Brightest point at ({x}, {y}) with brightness {brightness:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cpu_gpu.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All CPUs have multiple cores, ranging from 2 to 128\n",
    "    - Workhorse of scientific computing\n",
    "    - Each core has multiple threads which can take on different processes in parallel (Multithreading)\n",
    "    - Or you can use a whole core for one single process so that all the threads work on the same process (Multiprocessing) \n",
    "\n",
    "- Graphics cards (GPUs) can be used for scientific computing, which have up to 16k lightweight cores\n",
    "    - Very slow individually and have little cache memory\n",
    "    - But perfect for large simulations and vectorized message passing\n",
    "    - Difficult to program and handling CUDA exceptions is a pain in the butt\n",
    "    - Need to micro-manage memory movement, parallelism, using programming language CUDA \n",
    "    - When properly done, it can speed up calculations up to 50x, but in reality, it’s usually 10-20x\n",
    "    - Same speed-up expectations for other many-core CPU/DPU chips, but it’s easier to program. Doesn’t require a rewrite/refactor of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUT WHY???\n",
    "\n",
    "I present to you\n",
    "\n",
    "### THIS\n",
    "\n",
    " <img src=\"Frontier.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Multiple Python Instances\n",
    "\n",
    "<img src=\"gru.jpg\"/>\n",
    "\n",
    "This might sound very unsophisticated, but it sometimes is a decent choice. Python parallelism is not the best, and if tasks are completely independent of each other, running each task as a separate Python process and saving the result as a file is a perfectly reasonable (and simple) option. This is great for batch processes such as bulk processing a bunch of files with some data reduction code and is known as parameter sweep.\n",
    "\n",
    "There are many options to do this: bash script, GNU Parallel, or, as we will focus on today, a master python script. We will focus on using a python script to launch a bunch of python processes because all capabilities of shell scripting (e.g., calling bash commands with the sys module) can be done in Python, and often with much better readability. We will use python to launch a bunch of python processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Resource Usage\n",
    "\n",
    "Anytime you are testing parallelized code, it is good to monitor your CPU/RAM usage. Monitoring your CPU usage can help you assess if the number of processes being run in parallel is consistent with what you are looking for. Many times, running parallelized code already involves big data sets, and parallelization will use even more memory (you can think of it as trading runtime for memory usage). So have your resource monitor up occasionally when you are developing this code. It is also a great way to debug the code (e.g., identify hanging parallelization that is not finishing). \n",
    "\n",
    "We create a function with an argument `index` that tells each process what chunk of the task to run. We then create a bunch of processes, give them their chunks, and call `start()` to run them. Afterwards, our master process uses `join()` to wait for each process to finish. It is important to always call `join()` at the end to ensure all processes have finished running! If a process has finished immediately, `join()` will immediately return; if a process has not finished, our master process will sleep until the process it is waiting on has finished. \n",
    "\n",
    "### Before we begin with multiprocessing, a few important points:\n",
    "\n",
    " * Only one process can spawn other processes. Your subprocesses cannot spawn their own subprocesses! Generally, there is always a way to program things so that only one process needs to spawn subprocesses\n",
    " * While python is generally OS-agnostic, you may run into OS specific issues with multiprocesses due to the fact it is implemented a bit differently for Linux, Mac, and Windows. \n",
    " * One key difference to remember when developing packages for multiple OSes: on Mac and Windows, any lines with multiprocessing calls need to be wrapped in something like ``if __name__ == \"__main__\"``, otherwise you will get an error mentioning something about `freeze_support()` when trying to run it. This is not required in Jupyter notebooks, so we have merely commented where you would need them here. The details for why this is necessary is related to subprocesses not spawning their own subprocesses and is quite technical, but [here is a related blog post if you want to learn more](https://pythonspeed.com/articles/python-multiprocessing/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat = np.random.random((5000, 2000)) # 5000x2000 matrix, adjust size if this takes too long or runs too quickly\n",
    "\n",
    "def matrix_loop(mat, index):\n",
    "    # divide up so that we only compute one chunk of the mat.dot(mat.T) matrix\n",
    "    index_start = mat.shape[0] // 10 * index\n",
    "    index_end = mat.shape[0] // 10 * (index + 1)\n",
    "    val  = mat[index_start:index_end].dot(mat.T)\n",
    "    print(\"Process {0} complete. First value is {1}\".format(index, val.ravel()[0]))\n",
    "    # could save value to a shared variable or save to a file to be used later\n",
    "\n",
    "process_list = []\n",
    "\n",
    "### The following code would need to be wrapped in `if __name__ == \"__main__\":` (both for loops).\n",
    "for i in range(10):\n",
    "    p = mp.Process(target=matrix_loop, args=(mat, i))\n",
    "    process_list.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in process_list:\n",
    "    p.join()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pools\n",
    "\n",
    "Manually creating threads requires a bunch of upkeep code, which is unnecessary if you are just running over a giant loop. In the spirit of keeping parallelism simple, use a high-level parallelization API provided by your programming language whenever possible! It will save you time and effort (Trust me). For dividing up tasks with a for loop, use Python process `Pools`. Essentially, you can give\n",
    "any number of tasks to a process `Pool` and the processes in the pool will loop through and do each one per your instructions. \n",
    "\n",
    "When in doubt about how to parallelize code, use process pools! They are flexible and can accomodate most use cases. And since they have a standardized interface for how to use them, it will make your code more understadable by others compared to home-brewing your own parallelism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_pool(mat, index):\n",
    "    # divide up so that we only compute one chunk of the mat.dot(mat.T) matrix\n",
    "    index_start = mat.shape[0] // 10 * index\n",
    "    index_end = mat.shape[0] // 10 * (index + 1)\n",
    "    val  = mat[index_start:index_end].dot(mat.T)\n",
    "    print(\"Job {0} complete\".format(index))\n",
    "    \n",
    "    return val # let's return the data this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous parallelization\n",
    "\n",
    "### This entire block would need to be wrapped in `if __name__ == \"__main__\":`\n",
    "pool = mp.Pool(processes=8) # creae a pool with 8 worker processes\n",
    "\n",
    "pool_jobs = []\n",
    "for i in range(10):\n",
    "    job = pool.apply_async(matrix_pool, (mat, i))\n",
    "    pool_jobs.append(job)\n",
    "    print(\"Created job {0}\".format(i))\n",
    "\n",
    "for i, job in enumerate(pool_jobs):\n",
    "    result = job.get() \n",
    "    print(\"Job result {0}. First value is {1}\".format(i, result.ravel()[0]))\n",
    "\n",
    "# Close the pool and wait for the work to finish\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Synchronous parallelization\n",
    "\n",
    "### This entire block would need to be wrapped in `if __name__ == \"__main__\":`\n",
    "pool = mp.Pool(processes=8) # creae a pool with 8 worker processes\n",
    "\n",
    "pool_jobs = []\n",
    "for i in range(10):\n",
    "    result = pool.apply(matrix_pool, (mat, i))\n",
    "    pool_jobs.append(result)\n",
    "    print(\"Created job {0}\".format(i))\n",
    "\n",
    "for i, result in enumerate(pool_jobs):\n",
    "    print(\"Job result {0}. First value is {1}\".format(i, result.ravel()[0]))\n",
    "\n",
    "# Close the pool and wait for the work to finish\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries, but more like protocols\n",
    "#### Getting performance out of your code means\n",
    "- Picking the right algorithm\n",
    "- Implementing the algorithm efficiently\n",
    "- We also need to understand the computer’s CPU/GPU architecture and how it works\n",
    "- To exceed a single core’s performance, we must go parallel\n",
    "\n",
    "#### (Optional) Flynn’s taxonomy classifies computer architectures as single or multiple data; single or multiple instruction \n",
    "\n",
    "- Single instruction, single data (SISD)\n",
    "    - Typical application on your computer – no parallelism\n",
    "- Single instruction, multiple data (SIMD)\n",
    "    - The same instruction set is done to multiple pieces of data all at once\n",
    "    - Compile-time vectorization optimization; GPUs\n",
    "- Multiple instructions, single data (MISD)\n",
    "    - Not very interesting or useful\n",
    "- Multiple instructions, multiple data (MIMD)\n",
    "    - This is what we classify as parallel computing\n",
    "\n",
    "\n",
    "<img src=\"diff.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenMP\n",
    "\n",
    "OpenMP is a set of compiler directives and an API for C, C++, and Fortran that provides support for parallel programming in shared-memory environments. OpenMP is a portable, scalable model that gives programmers a simple and flexible interface for developing parallel applications for platforms ranging from the standard desktop computer to the supercomputer.\n",
    "\n",
    "- Most (all?) modern compilers support OpenMP, however, the performance across them can vary. GCC does a reasonable job.\n",
    "- Python enforces a “global interpreter lock” that means only one thread can talk to the interpreter at any one time. So, OpenMP within pure python is not possible.\n",
    "- However, C extensions or Cython code called from python can use shared-memory parallelism. That is, the underlying code can have OpenMP directives\n",
    "- There will be a systemwide default for OMP_NUM_THREADS. Things will still run if you use more threads than cores available, but don’t do this, it's not good practise!\n",
    "- This is called oversubscription, and will cause work to queue up. It’s better to use a maximum of 1 OpenMP thread per core\n",
    "- Scaling: if you double the number of cores, does the code take ½ the time?\n",
    "\n",
    "#### OPTIONAL\n",
    "\n",
    "```cpp\n",
    "#include <stdio.h>\n",
    "void main() {\n",
    "#pragma omp parallel\n",
    "printf(“Hello world\\n”);\n",
    "}\n",
    "```\n",
    "\n",
    "You can compile this with gcc –o hello –fopenmp hello.c\n",
    "\n",
    "```cpp\n",
    "#include <stdio.h>\n",
    "#ifdef _OPENMP\n",
    "#include <omp.h>\n",
    "#endif\n",
    "void main() \n",
    "{  \n",
    "    int nthreads, thread_num;\n",
    "\n",
    "#ifdef _OPENMP  \n",
    "    nthreads = omp_get_num_threads();  \n",
    "    thread_num = omp_get_thread_num();  \n",
    "    printf(\"[thread %d] Outside of parallel region = %d\\n\", thread_num, nthreads);\n",
    "#pragma omp parallel\n",
    "{  \n",
    "    nthreads = omp_get_num_threads();  \n",
    "    thread_num = omp_get_thread_num();  \n",
    "    printf(\"[thread %d] Inside of parallel region = %d\\n\", thread_num, nthreads);\n",
    "}\n",
    "#else  \n",
    "    printf(\"OpenMP not enabled.\\n\");\n",
    "#endif\n",
    "}\n",
    "\n",
    "#pragma omp parallel private(i,j,n)  \n",
    "{  \n",
    "    n = 0;  \n",
    "    #pragma omp for  \n",
    "    for (j = 0; j < N; j++) {    \n",
    "        for (i = 0; i < N; i++, n++) {      \n",
    "            a[n] = i + j;    \n",
    "    }    \n",
    "    x[j] = j;    \n",
    "    b[j] = 0.0;  \n",
    "}\n",
    "}\n",
    "\n",
    "// Multiply  \n",
    "n = 0; \n",
    "#pragma omp for  \n",
    "for (j = 0; j < N; j++) \n",
    "{    \n",
    "    for (i = 0; i < N; i++, n++)  \n",
    "        { b[i] = b[i] + a[n] * x[j];   \n",
    "    } \n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example time!\n",
    "\n",
    "Let's build a Monte Carlo solver that simulates radioactive decay that runs in serial (1 core) and in parallel, using Python's thread pool in the multiprocessing module.\n",
    "\n",
    "Consider the radioactive decay of ${ }^{8} \\mathrm{Li}$. This isotope of $\\mathrm{Li}$ is unstable and has a relatively long half-life $\\left(\\tau_{1 / 2}\\right)$ of $838.7(3) \\mathrm{ms} .{ }^{8} \\mathrm{Li}$ undergoes a $\\beta^{-}$ decay and produces daughter isotope of ${ }^{8} \\mathrm{Be}$. The decay rate can be described using a simple differential equation:\n",
    "\n",
    "$$\n",
    "\\frac{d N}{d t}=-\\lambda N\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the decay rate. By integrating Equation 1, we obtain:\n",
    "\n",
    "$$\n",
    "N=N_{0} e^{-\\lambda t}\n",
    "$$\n",
    "\n",
    "where $N_{0}$ is the initial number of particles. The decay rate, $\\lambda$, can be rewritten in terms of the half-life: $\\lambda=\\ln (2) / \\tau_{1 / 2}$.\n",
    "\n",
    "Function to model the radioactive decay of ${ }^{8} \\mathrm{Li}$ particles using the Metropolis algorithm. Consider a system with $N_{0}=1,000$ and model the decay up to a maximum time of 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N0 = 100000 # change this to 1000 if PC is old\n",
    "num_simulations = 100 # reduce this to 10 if PC is old\n",
    "half_life = 0.8387  \n",
    "max_time = 1\n",
    "dt = 0.001\n",
    "\n",
    "# Read more about metropolis decay algorithm here - https://blog.djnavarro.net/posts/2023-04-12_metropolis-hastings/\n",
    "\n",
    "def metropolis_decay(N0, half_life, max_time, dt):\n",
    "    decay_constant = np.log(2) / half_life\n",
    "    time_points = np.arange(0, max_time, dt)\n",
    "    num_steps = len(time_points)\n",
    "    decayed_particles = np.zeros(num_steps)\n",
    "    undecayed_particles = np.ones(num_steps) * N0\n",
    "\n",
    "    for i in range(1, num_steps):\n",
    "        undecayed_previous = int(undecayed_particles[i - 1])\n",
    "        decayed_previous = int(decayed_particles[i - 1])\n",
    "\n",
    "        decay_probability = 1 - np.exp(-decay_constant * dt)\n",
    "\n",
    "        random_numbers = np.random.rand(undecayed_previous)\n",
    "        decayed_this_step = random_numbers < decay_probability # Boolean array for decayed particles\n",
    "\n",
    "        undecayed_particles[i] = undecayed_previous - np.sum(decayed_this_step)\n",
    "        decayed_particles[i] = decayed_previous + np.sum(decayed_this_step)\n",
    "\n",
    "    return time_points, undecayed_particles, decayed_particles\n",
    "\n",
    "\n",
    "time_points, undecayed_particles, decayed_particles = metropolis_decay(N0, half_life, max_time, dt)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(time_points, undecayed_particles, label='Undecayed Particles')\n",
    "plt.plot(time_points, decayed_particles, label='Decayed Particles')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Number of Particles')\n",
    "plt.title('Radioactive Decay of $^8$Li')\n",
    "plt.xlim(0, max_time)\n",
    "plt.ylim(0, N0)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serial decay - running 100 systems of 100000 particle decays in series\n",
    "\n",
    "# Record execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Run simulations in serial\n",
    "for _ in range(num_simulations):\n",
    "    time_points, undecayed_particles, decayed_particles = metropolis_decay(N0, half_life, max_time, dt)\n",
    "    plt.plot(time_points, undecayed_particles)\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Number of Particles')\n",
    "plt.title(f'Radioactive Decay of $^8$Li for {num_simulations} Simulations')\n",
    "plt.xlim(0, max_time)\n",
    "plt.ylim(0, N0)\n",
    "plt.grid()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for {num_simulations} serial simulations: {execution_time:.4f} seconds\")\n",
    "plt.text(0.6, 100, f'Serial runtime: {execution_time:.4f} sec')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallel decay - running 100 systems of 100000 particle decays in parallel\n",
    "\n",
    "# Worker function for parallel simulations\n",
    "def worker(seed, N0, half_life, max_time, dt, output_dict):\n",
    "    np.random.seed(seed)\n",
    "    #complete the function\n",
    "\n",
    "num_processes = os.cpu_count()\n",
    "\n",
    "# set up simulations in parallel\n",
    "manager = mp.Manager()\n",
    "output_dict = manager.dict()\n",
    "\n",
    "# Record execution time using time library\n",
    "\n",
    "\n",
    "# Define Processes with a random seed for each process\n",
    "\n",
    "\n",
    "# You're missing something here\n",
    "\n",
    "\n",
    "\n",
    "all_time_points = []\n",
    "all_undecayed_particles = []\n",
    "all_decayed_particles = []\n",
    "\n",
    "for seed, result in output_dict.items():\n",
    "    #complete the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Number of Particles')\n",
    "plt.title(f'Radioactive Decay of $^8$Li for {num_simulations} Simulations')\n",
    "plt.xlim(0, max_time)\n",
    "plt.ylim(0, N0)\n",
    "plt.grid()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for {num_simulations} parallel simulations: {execution_time:.4f} seconds\")\n",
    "plt.text(0.6, 100, f'Parallel runtime: {execution_time:.4f} sec')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI\n",
    "\n",
    "The Message Passing Library (MPI) is the standard library for distributed parallel computing. Now each core cannot directly see each other’s memory. You need to manage how the work is divided and explicitly send and receive messages between MPI processes. A separate instance of your program is run on each core – these are MPI processes. Thread safety is not an issue here because each instance of the program is isolated from the others. The main idea in MPI is sending messages between processes.\n",
    "\n",
    "No longer do we simply use comments. We need to call routines in the MPI library.\n",
    "\n",
    "```cpp\n",
    "MPI_Init(&argc, &argv);  \n",
    "MPI_Comm_size(MPI_COMM_WORLD, &size);  \n",
    "MPI_Comm_rank(MPI_COMM_WORLD, &rank);  \n",
    "\n",
    "hostname[MAX_LENGTH-1] = '\\0';  \n",
    "gethostname(hostname, MAX_LENGTH-1);  \n",
    "printf(\"P%04d/%04d: Hello world from %s\\n\", rank, size, hostname);\n",
    "MPI_Finalize();\n",
    "```\n",
    "you can run this with mpirun –n 4 ./hello_mpi\n",
    "\n",
    "In python, we use Open-MPI, not to be confused with OpenMP, which is an open source implementation of MPI. Things are a bit more easier with OpenMPI but it still isn't easy to implement, but if you do, your code is much faster!\n",
    "\n",
    "More useful info - https://colab.research.google.com/drive/1yxusXcFQ9ea1bff4_q5iToMhGeQnmSwC?usp=sharing#scrollTo=zqOAtb4G4-e2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at an example where we parallelize an N-body solver that follows the 2D dynamics of a purely gravitational system with 1000 particles of 1 KG each distributed in a sphere of radius 1 meter that is bound to collapse in on itself.\n",
    "\n",
    "install mpi4py using pip install mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -n 1 python Nbody.py\n",
    "!mpirun -n 4 python Nbody.py\n",
    "!mpirun -n 8 python Nbody.py\n",
    "!mpirun -n 12 --oversubscribe python Nbody.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization routine is only run on the root (rank-0) MPI process. What MPI collective call to use to share those data with the other MPI processes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Given code\n",
    "comm = MPI.COMM_WORLD\n",
    "if comm.rank == 0:\n",
    "    pos, vel = initialize(Npart)\n",
    "else:\n",
    "    pos, vel = None, None\n",
    "\n",
    "pos, vel = comm.bcast((pos, vel), root=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parallelize the force calculation by splitting the particle position list and sending them to each core. How do we modify the routine subrange to calculate the starting and ending indices of the portion associated with each MPI process (rank)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def subrange(comm):\n",
    "    # Serial code\n",
    "    start = 0\n",
    "    end = Npart\n",
    "\n",
    "    # Parallel code\n",
    "    size = comm.Get_size()  # Total number of processes\n",
    "    rank = comm.Get_rank()  # Rank of the current process\n",
    "    # Distribute the particles evenly across all processes accounting for non-divisible cases\n",
    "    particles_per_process = Npart // size\n",
    "    remainder = Npart % size\n",
    "    if rank < remainder: #non-divisible case\n",
    "        start = rank * (particles_per_process + 1)\n",
    "        end = start + particles_per_process + 1\n",
    "    else: # divisble case\n",
    "        start = rank * particles_per_process + remainder\n",
    "        end = start + particles_per_process\n",
    "    return start, end, end-start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the calc_accel routine, each core only works on a subset of the particles, effectively parallelizing it. However, the new acceleration arrays are local to the MPI process on which they were calculated. How do we use an MPI collective call to share these results with all MPI processes and then combine these results into a single array with a shape $=(\\mathrm{N}, 3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#code for sharing results\n",
    "all_accel = np.empty([Npart, 3])\n",
    "comm.Allgather([accel, MPI.DOUBLE], [all_accel, MPI.DOUBLE])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the code with 2, 4, 8 and 10 cores and see how the performance scales. To help visualize, we will plot the parallel efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency metrics\n",
    "run_time = np.array([44.292, 154.678, 84.62, 133.810])\n",
    "steps = np.array([22.6, 3.2, 5.9, 3.7])\n",
    "num_cores = np.array([1, 4, 8, 10])\n",
    "speedup = (run_time[0] - run_time) / run_time[0] * 100\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_cores, run_time, marker='o', label='Run Time')\n",
    "for i, txt in enumerate(run_time):\n",
    "    plt.annotate(f'{txt:.2f}', (num_cores[i], run_time[i]))\n",
    "plt.xlabel('Number of Cores')\n",
    "plt.ylabel('Time taken (s)')\n",
    "plt.title('Efficiency of Parallelization - time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_cores, speedup, marker='D', label='Speedup')\n",
    "plt.plot(num_cores, steps, marker='*', label='Steps/sec')\n",
    "for i, txt in enumerate(speedup):\n",
    "    plt.annotate(f'{txt:.2f} %', (num_cores[i], speedup[i]))\n",
    "for i, txt in enumerate(steps):\n",
    "    plt.annotate(f'{txt:.2f}', (num_cores[i], steps[i]))\n",
    "plt.xlabel('Number of Cores')\n",
    "plt.ylabel('Efficiency')\n",
    "plt.title('Efficiency of Parallelization - metrics')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only parallelized the for-loop in the force calculation but left most of the other code untouched. From inspecting the performance increases, is it worthwhile to parallelize the other parts of the code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Debugging\n",
    "\n",
    "- There are parallel debuggers, but they are usually expensive\n",
    "- It’s possible to spawn multiple gdb (GNU debugger) sessions, but this gets out of hand quickly \n",
    "– “mpirun -n 2 xterm -e gdb ./a.out”\n",
    "- Print is still your friend - Run a small portion of a problem as possible on as few cores as necessary\n",
    "- Some round off differences are to be expected from sums (different order of operation ... numeric addition is not associative)\n",
    "\n",
    "Use this line of code to debug Nbody.py where the -np x is the number of parallel processes you want spawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 2 xterm -e gdb -ex run --args python3 Nbody.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid parallelism (Optional)\n",
    "\n",
    "<img src=\"mix.png\"/>\n",
    "\n",
    "- To get good performance on current HPC platforms (>1k cores), you need to use hybrid parallelism\n",
    "- OpenMP within a node / socket, MPI across nodes\n",
    "- For example, let's assume a huge 4x4x4 cube where you need to perform computations within a cube and across the cubes - accretion disk simulation and boundary condition passing\n",
    "- Then we have MPI to communicate across cubes and OpenMP within the cubes\n",
    "- The hybrid approach is often needed to get the best performance on big machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU parallelization (Optional)\n",
    "\n",
    "<img src=\"gpu_para.png\">\n",
    "\n",
    "- GPU offloading can greatly accelerate computing\n",
    "- Main issue: data needs to be transferred from the CPU/RAM across the PCIe bus (relatively slow) to the GPU\n",
    "- GPUs work as SIMD parallel machines (vectorized)\n",
    "    - The same instructions operate on all the data in lockstep\n",
    "    - Branching (if-statements) is slower\n",
    "- Best performance requires that you structure the code to be vectorized\n",
    "- CUDA is popular but is not portable to non-nVidia GPUs and accelerators\n",
    "- but OpenMP supports offloading to GPUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Image Processing\n",
    "\n",
    "Run the following code to generate some fake data and then run the following image processing function on each fake image and save the resulting data. How fast can you process the data when the images are 1000x1000? What is the speedup from 1 core to multiple cores (is it linear?)?\n",
    "\n",
    "### Instructions\n",
    "\n",
    "  * Run the first cell below to generate 25 images of fake data\n",
    "  * Run the second cell below to run the `process_image()` function on a single image without parallelism. Time how long this takes. Let's call this time t0. \n",
    "  * Write parallelization code to parallelize the `process_image()` function to process all 25 images. Time how long it takes to process 25 images. Let's call this tp. \n",
    "  * Compute speedup = (25 * t0) / tp: the speedup factor between doing 25 images in series and parallelizing it. \n",
    "  * (Optional) Try using different number of processes to parallelize the activity. How does the run time change with the number of processes used? How does it relate to the number of cores you have on your computer? Is it linear?\n",
    "\n",
    "### End Product\n",
    "  * Report on Piazza your best speedup factor you got. And specify how many processes you used, and how many cores you have.\n",
    "  * (Optional): make a graph of wall clock time to process 25 images vs number of processes used. Post this on Piazza as well.  \n",
    "\n",
    "### Roles\n",
    "  * Driver: in charge of sharing their screen and typing the code for this activity\n",
    "  * Recorder: in charge of writing down all the results and reporting back when we poll everyone \n",
    "  * Navigator: in charge of directing the driver what to code (everyone else; can be more than one person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, this cell might take a minute to run.\n",
    "\n",
    "def generate_fake_data(filename, dims):\n",
    "    \"\"\"\n",
    "    Generates a fake dataframe with random numbers\n",
    "    \n",
    "    Args:\n",
    "        filename (str): file to save the data to\n",
    "        dims (tuple): (Ny, Nx) pair that species the size of the y and x dimensions\n",
    "    \"\"\"\n",
    "    # some complicated random image generation. Feel free to ignore.\n",
    "    # coordinate system in fourier spae\n",
    "    u,v = np.meshgrid(np.fft.fftfreq(dims[1]), np.fft.fftfreq(dims[0]))\n",
    "    phases = np.random.uniform(0, 2*np.pi, u.shape)\n",
    "    rho = np.sqrt((u*dims[1])**2 + (v*dims[0])**2)\n",
    "    # suppress high frequency by a squared exponential\n",
    "    spectrum = np.exp(-rho**2/(np.max(rho)/50)**2)  * np.exp(1j * phases)\n",
    "    filtered = np.real(np.fft.ifft2(spectrum))\n",
    "\n",
    "    fits.writeto(filename, filtered, overwrite=True)\n",
    "\n",
    "\n",
    "# generate fake data (can choose to save it somethere else if you want)\n",
    "fileformat = os.path.join(\"./\", \"fake_{0}x{1}_{2}.fits\")\n",
    "\n",
    "ny = 1000\n",
    "nx = 1000\n",
    "for i in range(25):\n",
    "    filename = fileformat.format(ny, nx, i)\n",
    "    generate_fake_data(filename, (ny, nx) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(frame, filtersize=50):\n",
    "    \"\"\"\n",
    "    Run a high-pass filter on the data. \n",
    "    Remove the low spatial frequency (i.e., smooth features) in the image\n",
    "\n",
    "    Args:\n",
    "        frame (np.array): a 2-D image to be processed\n",
    "        fitersize (int): the size of the filter. Features smaller than the filtersize will be preserved\n",
    "\n",
    "    Returns:\n",
    "        processed_frame (np.array): a 2-D image after processing\n",
    "    \"\"\"\n",
    "    # run a median filter to smooth the image\n",
    "    frame_smooth = ndimage.median_filter(frame, filtersize)\n",
    "\n",
    "    processed_frame = frame - frame_smooth\n",
    "\n",
    "    return processed_frame\n",
    "\n",
    "# an example of running this on one image\n",
    "with fits.open(fileformat.format(ny, nx, 0)) as hdulist:\n",
    "    data = hdulist[0].data\n",
    "\n",
    "\n",
    "    filt_data = process_image(data)\n",
    "\n",
    "    fig = plt.figure(figsize=(6,3))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.imshow(data, cmap=\"inferno\")\n",
    "    ax1.set_title(\"Original\")\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.imshow(filt_data, cmap=\"inferno\")\n",
    "    ax2.set_title(\"Filtered\")\n",
    "\n",
    "#### Activity:\n",
    "# write and time some code that runs this on all 25 images in parallel. How does the performance increase \n",
    "# as you increase the number of processes you use?\n",
    "# we recommend you use multiprocessing pool for this task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism Extended Cut\n",
    "\n",
    "There is a lot of talk about in parallelism - we are just scratching the surface. The above concepts _should_ cover the majority of parallelism use cases. However, here are some good things to know.\n",
    "\n",
    "\n",
    "## Do you really need parallelism\n",
    "\n",
    "We already emphasized this above, but parallelism adds complexity. Try avoiding parallelization until it is necessary. If some code takes 10 minutes to run, but you only need to run it once per week, is it worth parallelizing? The programming and upkeep costs may not be worth it. Parallelism also makes it hard to debug: we cannot attach the python debugger on other Python processes meaning they often crash with no error message as to why. GNU Parallel (below) is an alternative to writing more complicated code.\n",
    "\n",
    "## GNU Parallel\n",
    "\n",
    "If you have a script that you want to run multiple times (e.g., on multiple files), you don't need to write Python parallelization. We can use the `parallel` package that can be installed on UNIX systems to handle the parallelization. For example, if we can process a single file by running on the command line:\n",
    "\n",
    "    python process.py file_01.fits\n",
    "\n",
    "Then we can run it on `file_01.fits` to `file_99.fits` by running on the command line:\n",
    "\n",
    "    parallel python process.py ::: file_{01..99}.fits\n",
    "\n",
    "\n",
    "## Threads vs Processes\n",
    "Python has two parallelization modules: `multiprocessing` and `multithreading`. What's the difference? Why do we only use `multiprocessing`?\n",
    "\n",
    "Threads and processes are both tasks that your computer CPUs run in parallel. Threads share the same memory, whereas processes do not. Processes have to communicate between each other using shared variables (see below) or via a slower I/O method (writing to files, communicating over websockets). Most languages use threads to parallelize computations because sharing the same memory is very convenient and saves on resources. However, Python has the \"Global Intepreter Lock\" (GIL) that allows only one thread to run at a time (for complicated consistency reasons). Generally, parallelism in astronomy involves computationally intensive tasks so only one of them being able to run at a time would defeat the purpose. That's why you will generally only see multiprocessing in astronomy-related software development.\n",
    "\n",
    "Threads are more frequently seen outside of astronomy, but it is generally useful for \"I/O bound tasks\" as opposed to the \"CPU bound tasks\" that we usually encounter in astronomy. Anytime you have tasks with a lot of sleeping/waiting time such as if you have multiple web API queries (e.g., multiple database queries) or waiting for files to be created, threads are a better choice because they use less memory and they can access all of your variables instead of having to define shared variables.\n",
    "\n",
    "The one notable exception is that `numpy` functions that call C code releases the GIL. This means if your code is dominated by `numpy` matrix operations such as dot product, then you can actually use threads with minimal increase in runtime.\n",
    "\n",
    "## Shared Variables\n",
    "\n",
    "Processes do not by default share memory. For processes to read/write/access the same variables, we have to declare shared memory. Python `multiprocessing` provides nearly all shared memory structure that you should be using: queues and arrays. Queues allow for interprocess communication. Arrays allow for large datasets to be accessed by multiple processes without needing to duplicate them (possibly saving a lot of memory usage). Use these special arrays and queues becasue they are automatically synchronized between processes and do not require synchronization code, which is generally low-level synchronization that we should avoid programming unless we absolutely need it. \n",
    "\n",
    "However, in most applications, you will find you won't need to do this at all because each task will operate on a separte chunk of data. We did not need to use it in any of the examples seen in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cluster.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('codeastro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "70da8eaca2f829bebd9ae4bfee73e2015b5a57abadfaaa95753587bc60143f91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
